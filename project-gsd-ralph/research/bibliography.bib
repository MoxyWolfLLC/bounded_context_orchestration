% Bibliography: GSD + Ralph Loop Framework
% Project: GSD Ralph
% Phase: 2 - Theoretical Framework
% Task: 2.5 - Bibliography Foundation
% Created: 2026-01-19

% ============================================
% SECTION 1: Context Window and Attention
% ============================================

@article{liu2023lost,
  title={Lost in the Middle: How Language Models Use Long Contexts},
  author={Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  note={Originally presented 2023. Demonstrated U-shaped attention curve.}
}

@inproceedings{peng2024yarn,
  title={{YaRN}: Efficient Context Window Extension of Large Language Models},
  author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shao, Enrico},
  booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2024},
  note={Extended Llama-2 context to 128K tokens.}
}

@article{gu2024mamba,
  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2024},
  note={Alternative to attention using Selective State Space Models.}
}

@inproceedings{wu2024hierarchical,
  title={{HMT}: Hierarchical Memory Transformer for Efficient Long Context Processing},
  author={Wu, Zifan and others},
  booktitle={Proceedings of NAACL},
  year={2025},
  note={Memory-efficient long context approach.}
}

@article{scaling2025attention,
  title={Scaling Context Requires Rethinking Attention},
  author={Various},
  journal={arXiv preprint},
  year={2025},
  note={Critique of quadratic attention for long contexts.}
}

@article{efficient2025attention,
  title={Efficient Attention Mechanisms for Large Language Models: A Survey},
  author={Various},
  journal={arXiv preprint},
  year={2025},
  note={Comprehensive survey of attention efficiency approaches.}
}

% ============================================
% SECTION 2: RAG and Memory Systems
% ============================================

@article{gao2024rag,
  title={Retrieval-Augmented Generation for Large Language Models: A Survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2024},
  note={Comprehensive RAG taxonomy: Naive, Advanced, Modular.}
}

@article{rag2025comprehensive,
  title={Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers},
  author={Various},
  journal={arXiv preprint},
  year={2025},
  note={Updated RAG survey with 2024-2025 advances.}
}

@article{rag2025systematic,
  title={A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions},
  author={Various},
  journal={arXiv preprint},
  year={2025},
  note={Over 1,200 RAG papers on arXiv in 2024.}
}

@article{memory2024survey,
  title={A Survey on the Memory Mechanism of Large Language Model-based Agents},
  author={Various},
  journal={ACM Transactions on Information Systems},
  year={2024},
  note={Comprehensive agent memory taxonomy.}
}

@article{memory2024multiagent,
  title={Memory in LLM-based Multi-agent Systems: Mechanisms, Challenges, and Collective Intelligence},
  author={Various},
  journal={TechRxiv preprint},
  year={2024},
  note={Multi-agent memory mechanisms survey.}
}

@article{memory2025agents,
  title={Memory in the Age of AI Agents: A Survey},
  author={Liu, Shichun and others},
  journal={arXiv preprint arXiv:2512.13564},
  year={2025},
  note={Comprehensive agent memory survey.}
}

@article{amem2025,
  title={{A-Mem}: Agentic Memory for LLM Agents},
  author={Various},
  journal={arXiv preprint arXiv:2502.12110},
  year={2025},
  note={Utility-based memory retention for agents.}
}

@misc{letta2025agentfile,
  title={Agent File (.af): An Open File Format for Serializing Stateful Agents},
  author={Letta},
  year={2025},
  howpublished={\url{https://www.letta.com}},
  note={File-based stateful agent format.}
}

% ============================================
% SECTION 3: Orchestration and Multi-Agent
% ============================================

@article{multiagent2025survey,
  title={A Survey of Multi-AI Agent Collaboration: Theories, Technologies and Applications},
  author={Various},
  booktitle={Proceedings of ACM DEAI},
  year={2025},
  note={Multi-agent collaboration theories survey.}
}

@misc{anthropic2025multiagent,
  title={How We Built Our Multi-Agent Research System},
  author={Anthropic},
  year={2025},
  howpublished={\url{https://www.anthropic.com/engineering/multi-agent-research-system}},
  note={Orchestrator-worker pattern implementation.}
}

@article{tdag2025,
  title={{TDAG}: A Multi-Agent Framework Based on Dynamic Task Decomposition and Agent Generation},
  author={Wang, et al.},
  journal={Neural Networks},
  volume={185},
  year={2025},
  note={Dynamic task decomposition framework.}
}

@article{agentic2025taxonomy,
  title={AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges},
  author={Various},
  journal={arXiv preprint},
  year={2025},
  note={Taxonomy of agentic AI systems.}
}

@article{humanai2025orchestrating,
  title={Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research Challenge},
  author={Various},
  journal={arXiv preprint arXiv:2510.02557},
  year={2025},
  note={Manager agent for human-AI coordination.}
}

% ============================================
% SECTION 4: Prompt Engineering and Chaining
% ============================================

@inproceedings{promptchaining2024acl,
  title={Prompt Chaining vs. Stepwise Prompts: A Controlled Comparison},
  author={Various},
  booktitle={Findings of ACL},
  year={2024},
  note={Empirical comparison showing chaining outperforms monolithic prompts.}
}

@inproceedings{promptchaining2024iui,
  title={A Prompt Chaining Framework for Long-Term Recall in LLM-Powered Intelligent Assistants},
  author={Various},
  booktitle={Proceedings of IUI},
  year={2024},
  note={Chaining for context retention in assistants.}
}

@article{multistep2024survey,
  title={Multi-Step Reasoning with Large Language Models: A Survey},
  author={Various},
  journal={ACM Computing Surveys},
  year={2024},
  note={Comprehensive taxonomy of reasoning approaches.}
}

@article{reasoning2025limits,
  title={Reasoning Beyond Limits: Advances and Open Problems for LLMs},
  author={Various},
  journal={ScienceDirect},
  year={2025},
  note={Future challenges including multi-step reasoning robustness.}
}

% ============================================
% SECTION 5: Additional References
% ============================================

@misc{autogen2024,
  title={{AutoGen}: Enabling Next-Generation LLM Applications via Multi-Agent Conversation Framework},
  author={Microsoft Research},
  year={2024},
  howpublished={\url{https://github.com/microsoft/autogen}},
  note={Research-driven multi-agent framework.}
}

@article{graphrag2024,
  title={{GraphRAG}: Unlocking LLM Discovery on Narrative Private Data},
  author={Microsoft Research},
  journal={arXiv preprint},
  year={2024},
  note={Graph-based RAG for semantic gap.}
}

@misc{googlea2a2025,
  title={Agent-to-Agent (A2A) Protocol},
  author={Google},
  year={2025},
  note={Multi-agent interoperability standard.}
}

% ============================================
% COMPLETION STATUS
% ============================================

% Task 2.5: Bibliography Foundation - BIBLIOGRAPHY_STARTED
% Sources: 25 entries across 5 categories
% Categories: Context/Attention, RAG/Memory, Orchestration, Prompting, Additional
% Ready for: bibtex-abstract-generator if abstracts needed

